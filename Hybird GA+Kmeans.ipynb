{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2549419,"sourceType":"datasetVersion","datasetId":1546318},{"sourceId":7396106,"sourceType":"datasetVersion","datasetId":4300228}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *IMPORTING LIBRARIES*","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt, numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import metrics\nimport warnings\nimport sys\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nnp.random.seed(42)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import pairwise_distances","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *LOADING DATA*","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/customer-personality-analysis/marketing_campaign.csv\", sep=\"\\t\")\nprint(\"Number of datapoints:\", len(data))\ndata.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *DATA CLEANING*","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#To remove the NA values\ndata = data.dropna()\nprint(\"The total number of data-points after removing the rows with missing values are:\", len(data))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[\"Dt_Customer\"] = pd.to_datetime(data[\"Dt_Customer\"], dayfirst=True)\ndates = []\nfor i in data[\"Dt_Customer\"]:\n    i = i.date()\n    dates.append(i)  \n#Dates of the newest and oldest recorded customer\nprint(\"The newest customer's enrolment date in therecords:\",max(dates))\nprint(\"The oldest customer's enrolment date in the records:\",min(dates))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# how many days each customer has been a customer \n#relative to the most recent customer's enrollment date.\n#Created a feature \"Customer_For\"\ndays = []\nd1 = max(dates) #taking it to be the newest customer\nfor i in dates:\n    delta = d1 - i\n    days.append(delta)\ndata[\"Customer_For\"] = days\ndata[\"Customer_For\"] = pd.to_numeric(data[\"Customer_For\"], errors=\"coerce\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Total categories in the feature Marital_Status:\\n\", data[\"Marital_Status\"].value_counts(), \"\\n\")\nprint(\"Total categories in the feature Education:\\n\", data[\"Education\"].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Feature Engineering\n#Age of customer today \ndata[\"Age\"] = 2025-data[\"Year_Birth\"]\n\n#Total spendings on various items\ndata[\"Spent\"] = data[\"MntWines\"]+ data[\"MntFruits\"]+ data[\"MntMeatProducts\"]+ data[\"MntFishProducts\"]+ data[\"MntSweetProducts\"]+ data[\"MntGoldProds\"]\n\n#Deriving living situation by marital status\"Alone\"\ndata[\"Living_With\"]=data[\"Marital_Status\"].replace({\"Married\":\"Partner\", \"Together\":\"Partner\", \"Absurd\":\"Alone\", \"Widow\":\"Alone\", \"YOLO\":\"Alone\", \"Divorced\":\"Alone\", \"Single\":\"Alone\",})\n\n#Feature indicating total children living in the household\ndata[\"Children\"]=data[\"Kidhome\"]+data[\"Teenhome\"]\n\n#Feature for total members in the householde\ndata[\"Family_Size\"] = data[\"Living_With\"].replace({\"Alone\": 1, \"Partner\":2})+ data[\"Children\"]\n\n#Feature pertaining parenthood\ndata[\"Is_Parent\"] = np.where(data.Children> 0, 1, 0)\n\n#Segmenting education levels in three groups\ndata[\"Education\"]=data[\"Education\"].replace({\"Basic\":\"Undergraduate\",\"2n Cycle\":\"Undergraduate\", \"Graduation\":\"Graduate\", \"Master\":\"Postgraduate\", \"PhD\":\"Postgraduate\"})\n\n#For clarity\ndata=data.rename(columns={\"MntWines\": \"Wines\",\"MntFruits\":\"Fruits\",\"MntMeatProducts\":\"Meat\",\"MntFishProducts\":\"Fish\",\"MntSweetProducts\":\"Sweets\",\"MntGoldProds\":\"Gold\"})\n\n#Dropping some of the redundant features\nto_drop = [\"Marital_Status\", \"Dt_Customer\", \"Z_CostContact\", \"Z_Revenue\", \"Year_Birth\", \"ID\"]\ndata = data.drop(to_drop, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#To plot some selected features \n#Setting up colors prefrences\nsns.set(rc={\"axes.facecolor\":\"#FFF9ED\",\"figure.facecolor\":\"#FFF9ED\"})\npallet = [\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"]\ncmap = colors.ListedColormap([\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"])\n#Plotting following features\nTo_Plot = [ \"Income\", \"Recency\", \"Customer_For\", \"Age\", \"Spent\", \"Is_Parent\"]\nprint(\"Reletive Plot Of Some Selected Features: A Data Subset\")\nplt.figure()\nsns.pairplot(data[To_Plot], hue= \"Is_Parent\",palette= ([\"#682F2F\",\"#F3AB60\"]))\n#Taking hue \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Dropping the outliers by setting a cap on Age and income. \ndata = data[(data[\"Age\"]<90)]\ndata = data[(data[\"Income\"]<600000)]\nprint(\"The total number of data-points after removing the outliers are:\", len(data))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 0: Store original length\n# original_len = len(data)\n\n# # Step 1: Select only numeric columns\n# numeric_cols = data.select_dtypes(include=['number']).columns\n\n# # Step 2: Remove outliers using IQR for each numeric column\n# for col in numeric_cols:\n#     Q1 = data[col].quantile(0.25)\n#     Q3 = data[col].quantile(0.75)\n#     IQR = Q3 - Q1\n#     lower_bound = Q1 - 1.5 * IQR\n#     upper_bound = Q3 + 1.5 * IQR\n#     data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]\n\n# # Step 3: Compare lengths\n# new_len = len(data)\n# print(\"Original number of data points:\", original_len)\n# print(\"Number of data points after removing outliers:\", new_len)\n# print(\"Number of data points removed:\", original_len - new_len)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#correlation matrix\ncorrmat = data.corr(numeric_only=True)\nplt.figure(figsize=(20,20))  \nsns.heatmap(corrmat, annot=True, cmap=cmap, center=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\nprint(data.isnull().sum())\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *DATA PREPROCESSING*","metadata":{}},{"cell_type":"code","source":"#Get list of categorical variables\ns = (data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables in the dataset:\", object_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# import pandas as pd\n\n# # Drop irrelevant features first\n# cols_del = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n#             'AcceptedCmp2', 'Complain', 'Response']\n# ds = data.drop(cols_del, axis=1)\n\n# # One-hot encode categorical columns\n# ds = pd.get_dummies(ds, columns=object_cols, drop_first=True)\n\n# # Now scale the numeric features\n# scaler = StandardScaler()\n# scaled_ds = pd.DataFrame(scaler.fit_transform(ds), columns=ds.columns)\n\n# print(\"All features are now one-hot encoded and scaled.\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Label Encoding the object dtypes.\nLE=LabelEncoder()\nfor i in object_cols:\n    data[i]=data[[i]].apply(LE.fit_transform)\n    \nprint(\"All features are now numerical\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating a copy of data\nds = data.copy()\n# creating a subset of dataframe by dropping the features on deals accepted and promotions\ncols_del = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response','Kidhome', 'Teenhome', 'Is_Parent','Living_With','Education']\nds = ds.drop(cols_del, axis=1)\n#Scaling\nscaler = StandardScaler()\nscaler.fit(ds)\nscaled_ds = pd.DataFrame(scaler.transform(ds),columns= ds.columns )\nprint(\"All features are now scaled\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_del = ['Kidhome', 'Teenhome', 'Family_Size', 'Is_Parent','Living_With', 'Sweets', 'Fish', 'Education']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Scaled data to be used for reducing the dimensionality\nprint(\"Dataframe to be used for further modelling:\")\nscaled_ds.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *DIMENSIONALITY REDUCTION*","metadata":{}},{"cell_type":"code","source":"X_scaled = scaled_ds.copy()   # Full scaled features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Initiating PCA to reduce dimentions aka features to 3\npca = PCA(n_components=3)\npca.fit(X_scaled)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PCA_ds = pd.DataFrame(pca.transform(scaled_ds), columns=([\"col1\",\"col2\", \"col3\"]))\nPCA_ds.describe().T","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pca = PCA(n_components=3)\n# reduced = pca.fit_transform(X_scaled)\n# plt.scatter(reduced[:, 0], reduced[:, 1])\n# plt.title(\"PCA Projection of Scaled Data\")\n# plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.manifold import TSNE\n# tsne = TSNE(n_components=3, random_state=42)\n# tsne_result = tsne.fit_transform(X_scaled)\n# plt.scatter(tsne_result[:, 0], tsne_result[:, 1])\n# plt.title(\"t-SNE Projection\")\n# plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#A 3D Projection Of Data In The Reduced Dimension\nx =PCA_ds[\"col1\"]\ny =PCA_ds[\"col2\"]\nz =PCA_ds[\"col3\"]\n#To plot\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111, projection=\"3d\")\nax.scatter(x,y,z, c=\"maroon\", marker=\"o\" )\nax.set_title(\"A 3D Projection Of Data In The Reduced Dimension\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 0: Store original length\n# original_len = len(data)\n\n# # Step 1: Select only numeric columns\n# numeric_cols = data.select_dtypes(include=['number']).columns\n\n# # Step 2: Remove outliers using IQR for each numeric column\n# for col in numeric_cols:\n#     Q1 = data[col].quantile(0.25)\n#     Q3 = data[col].quantile(0.75)\n#     IQR = Q3 - Q1\n#     lower_bound = Q1 - 1.5 * IQR\n#     upper_bound = Q3 + 1.5 * IQR\n#     data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]\n\n# # Step 3: Compare lengths\n# new_len = len(data)\n# print(\"Original number of data points:\", original_len)\n# print(\"Number of data points after removing outliers:\", new_len)\n# print(\"Number of data points removed:\", original_len - new_len)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_scaled = scaled_ds.copy()   # Full scaled features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.cluster import KMeans  \n# from sklearn.metrics import silhouette_samples, silhouette_score  \n# import matplotlib.pyplot as plt  \n# import numpy as np  \n\n# range_n_clusters = range(2, 10)  \n# silhouette_scores = []  \n\n# for n_clusters in range_n_clusters:  \n#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)  \n#     cluster_labels = kmeans.fit_predict(X_scaled)  \n#     silhouette_avg = silhouette_score(X_scaled, cluster_labels)  \n#     silhouette_scores.append(silhouette_avg)  \n#     print(f\"For n_clusters={n_clusters}, Silhouette Score={silhouette_avg:.2f}\")  \n\n# # Plot the results  \n# plt.plot(range_n_clusters, silhouette_scores, marker='o')  \n# plt.xlabel(\"Number of Clusters\")  \n# plt.ylabel(\"Silhouette Score\")  \n# plt.show()  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hybrid GA + KMeans Clustering","metadata":{}},{"cell_type":"code","source":"# 1. Import required libraries\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import pairwise_distances\nfrom deap import base, creator, tools, algorithms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics import silhouette_score\nfrom collections import Counter\nfrom scipy.spatial.distance import pdist\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NUM_CLUSTERS = 5             # Number of clusters you want\n# POPULATION_SIZE = 100                     # Number of individuals in population\n# NUM_GENERATIONS = 500             # Number of generations\n# MUTATION_RATE = 0.01              # Probability of mutation\n# TOURNAMENT_SIZE = 5             # For tournament selection\n# MIN_SAMPLES_PER_CLUSTER = 5  # Minimum points required per cluster# Configuration parameters                     \n# ELITISM_RATE = 0.10                   # Percentage of elites to keep\n# TOURNAMENT_SIZE_EARLY = 5             # Tournament size early generations\n# TOURNAMENT_SIZE_LATE = 7              # Tournament size late generations\n# LOCAL_SEARCH_FREQUENCY = 20           # Run local search every N generations\n# DIVERSITY_CHECK_FREQUENCY = 25        # Check diversity every N generations\n# NUM_CLUSTERS = 5             # Number of clusters you want\n# POPULATION_SIZE = 200                     # Number of individuals in population\n# NUM_GENERATIONS = 50           # Number of generations\nMUTATION_RATE = 0.02              # Probability of mutation\nTOURNAMENT_SIZE = 2             # For tournament selection\nMIN_SAMPLES_PER_CLUSTER = 6  # Minimum points required per cluster# Configuration parameters                     \nELITISM_RATE = 0.2                   # Percentage of elites to keep\nTOURNAMENT_SIZE_EARLY = 2             # Tournament size early generations\nTOURNAMENT_SIZE_LATE = 4              # Tournament size late generations\nLOCAL_SEARCH_FREQUENCY = 30           # Run local search every N generations\nDIVERSITY_CHECK_FREQUENCY = 10        # Check diversity every N generations\nPENALTY_VALUE = 1000  # or any value that you want to use as a penalty\nMAX_GENERATIONS = 50\nNUM_CLUSTERS = 3\nPOPULATION_SIZE = 200\nNUM_GENERATIONS = 50\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fitness cache for memoization\nfitness_cache = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_scaled = scaled_ds.copy()   # Full scaled features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NUM_FEATURES = X_scaled.shape[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(NUM_FEATURES)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = scaled_ds.values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#For demonstration, we'll create synthetic data\n# X_scaled = StandardScaler().fit_transform(X)\nnp.random.seed(42)\nNUM_FEATURES = X_scaled.shape[1] \n# X = np.concatenate([\n#     np.random.normal(0, 1, (100, NUM_FEATURES)),\n#     np.random.normal(5, 1, (100, NUM_FEATURES)),\n#     np.random.normal(10, 1, (100, NUM_FEATURES))\n# ])\nX=X_scaled","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_scaled)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INDIVIDUAL_LENGTH = NUM_CLUSTERS * NUM_FEATURES","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmax_generations=50\n# def create_individual(generation=0, max_generations=200):\n#     \"\"\"Enhanced initialization combining K-Means++, random, and subspace sampling\"\"\"\n#     # Method 1: K-Means++ initialization (70% probability in early generations)\n#     if generation < max_generations * 0.2 and random.random() < 0.7:  \n#         if random.random() < 0.7:  # 70% chance for smart initialization\n            \n#             # Improved K-Means++ with subspace sampling\n#             subspace_size = max(3, NUM_FEATURES // 2)\n#             features = random.sample(range(NUM_FEATURES), subspace_size)\n#             subspace_X = X[:, features]\n            \n#             centroids = [subspace_X[random.randint(0, len(X)-1)]]\n#             for _ in range(1, NUM_CLUSTERS):\n#                 distances = np.array([min(np.linalg.norm(x - c) for c in centroids) for x in subspace_X])\n#                 probs = distances / (distances.sum() + 1e-8)\n#                 new_idx = np.random.choice(len(X), p=probs)\n#                 centroids.append(subspace_X[new_idx])\n            \n#             # Project back to full space\n#             full_centroids = np.zeros((NUM_CLUSTERS, NUM_FEATURES))\n#             full_centroids[:, features] = np.array(centroids)\n#             return full_centroids.flatten()\n#         else:  # 30% random with noise\n#             return (X_scaled[np.random.choice(len(X), NUM_CLUSTERS)] + \n#                     np.random.normal(0, 0.5, (NUM_CLUSTERS, NUM_FEATURES))).flatten()\n#     else:  # Later generations - more exploitation\n#         return (X_scaled[random.sample(range(len(X)), NUM_CLUSTERS)] + \n#                 np.random.normal(0, 0.05, (NUM_CLUSTERS, NUM_FEATURES))).flatten()\n\n\ndef create_individual(generation=0, max_generations=200):\n    \"\"\"Enhanced initialization combining K-Means++ and data-driven sampling\"\"\"\n    # Method 1: K-Means++ initialization (70% probability in early generations)\n    if generation < max_generations * 0.2 and random.random() < 0.7:\n        indices = [np.random.choice(len(X))]\n        for _ in range(1, NUM_CLUSTERS):\n            dists = np.min(cdist(X, X[indices]), axis=1)\n            probs = dists / (dists.sum() + 1e-8)  # Add small constant to avoid division by zero\n            indices.append(np.random.choice(len(X), p=probs))\n        return X[indices].flatten()\n    \n    # Method 2: Random data points with decreasing noise\n    noise_scale = 0.5 if generation < max_generations * 0.5 else 0.1\n    return (X[np.random.choice(len(X), NUM_CLUSTERS, replace=False)] + \n            np.random.normal(0, noise_scale, (NUM_CLUSTERS, NUM_FEATURES))).flatten()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def repair_individual(individual, X):\n    \"\"\"Ensure all clusters have at least MIN_SAMPLES_PER_CLUSTER points\"\"\"\n    centroids = np.array(individual).reshape(NUM_CLUSTERS, NUM_FEATURES)\n    distances = cdist(X, centroids, 'euclidean')\n    labels = np.argmin(distances, axis=1)\n    cluster_counts = np.bincount(labels, minlength=NUM_CLUSTERS)\n    \n    # Repair empty/small clusters\n    for i in range(NUM_CLUSTERS):\n        if cluster_counts[i] < MIN_SAMPLES_PER_CLUSTER:\n            # Find the farthest point from all centroids in this cluster's region\n            if cluster_counts[i] > 0:  # If cluster has some points\n                cluster_points = X[labels == i]\n                centroid_dist = cdist(cluster_points, [centroids[i]])\n                farthest_idx = np.argmax(centroid_dist)\n                centroids[i] = cluster_points[farthest_idx]\n            else:  # For completely empty clusters\n                centroids[i] = X[np.random.choice(len(X))].copy()\n    \n    return centroids.flatten()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_population(POPULATION_SIZE, generation=0, max_generations=200):\n    \"\"\"Create initial population with enhanced initialization\"\"\"\n    # return  np.array([create_individual(generation, max_generations) for _ in range(POPULATION_SIZE)])##i think i have to kep it not comment but try\n    return [repair_individual(create_individual(generation, max_generations) ,X) for _ in range(POPULATION_SIZE)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Fitness cache for memoization\n# fitness_cache = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##thi comment for me i change here the X to X_scaled\n# def evaluate_fitness(individual,generation,population=None, apply_sharing=False):\n#     \"\"\"Enhanced fitness evaluation with multiple criteria\"\"\"\n#     individual_tuple = tuple(individual)\n#     if individual_tuple in fitness_cache:\n#         return fitness_cache[individual_tuple]\n    \n#     # Initialize with penalty value by default\n#     PENALTY_VALUE = 1000\n#     final_fitness = PENALTY_VALUE  # Default penalty value\n    \n#     try:\n#         centroids = np.array(individual).reshape(NUM_CLUSTERS, NUM_FEATURES)\n#         distances = cdist(X, centroids, 'euclidean')\n#         labels = np.argmin(distances, axis=1).astype(int) \n        \n#         # Check cluster validity\n#         cluster_counts = np.bincount(labels, minlength=NUM_CLUSTERS)\n#         unique_clusters = np.unique(labels)\n        \n#         if (len(cluster_counts) < 2 or\n#            np.any(cluster_counts < MIN_SAMPLES_PER_CLUSTER)):\n#            return PENALTY_VALUE\n\n        \n#         # Calculate fitness components\n#         sil_score = silhouette_score(X, labels)\n        \n#         intra_cluster = np.mean([np.mean(distances[labels == i, i]) \n#                                for i in range(NUM_CLUSTERS)if cluster_counts[i] > 0])\n        \n        \n#         # inter_cluster = np.min(pairwise_distances(centroids))\n#         inter_cluster = np.min(pdist(centroids))\n        \n#         balance_penalty = np.std(cluster_counts) / len(X)\n        \n#         # Calculate base fitness\n#         base_fitness = - (0.7 * sil_score + \n#                           0.2 * (1/intra_cluster) + \n#                           0.1 * inter_cluster - \n#                           0.2 * balance_penalty-\n#                           0.1 * (NUM_CLUSTERS - len(unique_clusters)))\n        \n#         # Apply fitness sharing if enabled\n#         if apply_sharing and population is not None:\n#             population = np.array(population) \n#             # sigma_share = 0.3 * NUM_FEATURES\n#             sigma_share = (0.25 + 0.15 * (generation/max_generations)) * NUM_FEATURES  # Starts broad (0.4), narrows to 0.25\n#             alpha = 1\n#             share_sum = 1.0\n            \n\n#             neighbor_sample_size = min(\n#                 max(20, int(0.3 * len(population))),  # At least 20, or 30% of population\n#                 len(population)\n#             )\n            \n#             for other in random.sample(population, neighbor_sample_size):\n#                 distance = np.linalg.norm(np.array(individual) - population_array,\n#         axis=1\n#     )\n#                 if distance < sigma_share:\n#                     share_sum += 1 - (distance/sigma_share)**alpha\n                    \n                    \n            \n#             final_fitness = base_fitness / share_sum\n#         else:\n#             final_fitness = base_fitness\n            \n#     except Exception as e:\n#         print(f\"Error in fitness evaluation: {str(e)}\")\n#         final_fitness = PENALTY_VALUE\n    \n#     fitness_cache[individual_tuple] = final_fitness\n#     return final_fitness","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##the code run without error but give low score 0.3\n# ##thi comment for me i change here the X to X_scaled\n# def evaluate_fitness(individual,generation,population=None, apply_sharing=False):\n#     \"\"\"Enhanced fitness evaluation with multiple criteria\"\"\"\n#     individual_tuple = tuple(individual)\n#     if individual_tuple in fitness_cache:\n#         return fitness_cache[individual_tuple]\n    \n#     # Initialize with penalty value by default\n#     PENALTY_VALUE = 1000\n#     final_fitness = PENALTY_VALUE  # Default penalty value\n    \n#     try:\n#         centroids = np.array(individual).reshape(NUM_CLUSTERS, NUM_FEATURES)\n#         distances = cdist(X, centroids, 'euclidean')\n#         labels = np.argmin(distances, axis=1).astype(int) \n        \n#         # Check cluster validity\n#         cluster_counts = np.bincount(labels, minlength=NUM_CLUSTERS)\n#         unique_clusters = np.unique(labels)\n        \n#         if (len(cluster_counts) < 2 or\n#            np.any(cluster_counts < MIN_SAMPLES_PER_CLUSTER)):\n#            return PENALTY_VALUE\n\n        \n#         # Calculate fitness components\n#         sil_score = silhouette_score(X, labels)\n        \n#         intra_cluster = np.mean([np.mean(distances[labels == i, i]) \n#                                for i in range(NUM_CLUSTERS)if cluster_counts[i] > 0])\n        \n        \n#         # inter_cluster = np.min(pairwise_distances(centroids))\n#         inter_cluster = np.min(pdist(centroids))\n        \n#         balance_penalty = np.std(cluster_counts) / len(X)\n        \n#         # Calculate base fitness\n#         base_fitness = - (0.7 * sil_score + \n#                           0.2 * (1/intra_cluster) + \n#                           0.1 * inter_cluster - \n#                           0.2 * balance_penalty-\n#                           0.1 * (NUM_CLUSTERS - len(unique_clusters)))\n        \n#         # Apply fitness sharing if enabled\n#         if apply_sharing and population is not None:\n#             population = np.array(population) \n#             # sigma_share = 0.3 * NUM_FEATURES\n#             sigma_share = (0.25 + 0.15 * (generation/max_generations)) * NUM_FEATURES  # Starts broad (0.4), narrows to 0.25\n#             alpha = 1\n#             share_sum = 1.0\n            \n\n#             neighbor_sample_size = min(\n#                 max(20, int(0.3 * len(population))),  # At least 20, or 30% of population\n#                 len(population)\n#             )\n            \n#             for other in random.sample(population, neighbor_sample_size):\n#                 distance = np.linalg.norm(np.array(individual) - population_array,\n#         axis=1\n#     )\n#                 if distance < sigma_share:\n#                     share_sum += 1 - (distance/sigma_share)**alpha\n                    \n                    \n            \n#             final_fitness = base_fitness / share_sum\n#         else:\n#             final_fitness = base_fitness\n            \n#     except Exception as e:\n#         print(f\"Error in fitness evaluation: {str(e)}\")\n#         final_fitness = PENALTY_VALUE\n    \n#     fitness_cache[individual_tuple] = final_fitness\n#     return final_fitness","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_fitness(individual, population=None, apply_sharing=False):\n    global fitness_cache\n    \"\"\"Enhanced fitness evaluation with multiple criteria\"\"\"\n    individual_tuple = tuple(individual)\n    if individual_tuple in fitness_cache:\n        return fitness_cache[individual_tuple]\n    \n    PENALTY_VALUE = 1000\n    final_fitness = PENALTY_VALUE\n    \n    try:\n        centroids = np.array(individual).reshape(NUM_CLUSTERS, NUM_FEATURES)\n        distances = cdist(X, centroids, 'euclidean')\n        labels = np.argmin(distances, axis=1).astype(int)\n        \n        # Cluster validity checks\n        cluster_counts = np.bincount(labels, minlength=NUM_CLUSTERS)\n        unique_clusters = np.unique(labels)\n        \n        if (len(unique_clusters) < 2 or \n            np.any(cluster_counts < MIN_SAMPLES_PER_CLUSTER)):\n            return PENALTY_VALUE\n        \n        # Calculate metrics\n        sil_score = silhouette_score(X, labels)\n        intra_cluster = np.mean([np.mean(distances[labels == i, i]) \n                               for i in range(NUM_CLUSTERS) if cluster_counts[i] > 0])\n        inter_cluster = np.min(pdist(centroids))if NUM_CLUSTERS > 1 else 0\n        balance_penalty = np.std(cluster_counts) / len(X)\n        \n        # Optimized weight distribution\n        base_fitness = - (0.85 * sil_score + \n                         0.10 * (1/intra_cluster) + \n                         0.05 * inter_cluster - \n                         0.02 * balance_penalty - \n                         0.05 * (NUM_CLUSTERS - len(unique_clusters)))\n        \n        # Fitness sharing if enabled\n        if apply_sharing and population is not None:\n            sigma_share = 0.3 * NUM_FEATURES\n            alpha = 1\n            share_sum = 1.0\n            \n            for other in random.sample(population, min(20, len(population))):\n                distance = np.linalg.norm(np.array(individual) - np.array(other))\n                if distance < sigma_share:\n                    share_sum += 1 - (distance/sigma_share)**alpha\n            \n            final_fitness = base_fitness / share_sum\n        else:\n            final_fitness = base_fitness\n            \n    except Exception as e:\n        print(f\"Error in fitness evaluation: {str(e)}\")\n        final_fitness = PENALTY_VALUE\n    \n    fitness_cache[individual_tuple] = final_fitness\n    return final_fitness","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tournament_selection(population, fitnesses, generation, max_generations, tournament_size=3, elitism=0.15):\n    # Gradually increase tournament size\n    min_size = 3\n    max_size = 7\n    diversity = np.mean(pairwise_distances(population))\n    tournament_size = min_size + int((generation/max_generations) * (max_size-min_size))\n    \n    # Keep some randomness in selection pressure\n    if random.random() < 0.2:  # 20% chance to vary\n        tournament_size = random.randint(min_size, max_size)\n        \n    selected = []\n    elite_count = int(ELITISM_RATE * len(population))\n    \n    # Elitism: select top elite_count individuals\n    if elite_count > 0:\n        elite_indices = np.argpartition(fitnesses, elite_count)[:elite_count]\n        elites = [population[i] for i in elite_indices]\n        selected.extend(elites)\n    \n    # Tournament selection for the rest\n    for _ in range(len(population) - elite_count):\n        participants = random.sample(list(zip(population, fitnesses)), tournament_size)\n        winner = min(participants, key=lambda x: x[1])  # Assuming minimization\n        selected.append(winner[0])\n    \n    return selected\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crossover(parent1, parent2, generation, max_generations):\n    \"\"\"SBX crossover returning both children\"\"\"\n    crossover_rate = 0.9 - (0.5 * generation/max_generations)\n    if random.random() > crossover_rate:\n        return parent1.copy(), parent2.copy()\n    \n    p1 = np.array(parent1).reshape(NUM_CLUSTERS, NUM_FEATURES)\n    p2 = np.array(parent2).reshape(NUM_CLUSTERS, NUM_FEATURES)\n    \n    child1 = np.zeros_like(p1)\n    child2 = np.zeros_like(p1)\n    eta = random.uniform(2, 10)\n\n    for k in range(NUM_CLUSTERS):\n        for d in range(NUM_FEATURES):\n            u = random.random()\n            beta = (2*u)**(1/(eta+1)) if u <= 0.5 else (1/(2*(1-u)))**(1/(eta+1))\n            child1[k,d] = 0.5*((1+beta)*p1[k,d] + (1-beta)*p2[k,d])\n            child2[k,d] = 0.5*((1-beta)*p1[k,d] + (1+beta)*p2[k,d])\n\n    child1 = np.clip(child1, X.min(axis=0), X.max(axis=0)).flatten()\n    child2 = np.clip(child2, X.min(axis=0), X.max(axis=0)).flatten()\n    \n    return child1, child2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef mutate(individual, generation, max_generations):\n    \n    individual = np.array(individual).reshape(NUM_CLUSTERS, NUM_FEATURES)##aff reshape\n    \n    # base_rate = 0.2 if generation < max_generations*0.3 else 0.1\n    # mutation_rate = base_rate + (0.3 * (1 - generation/max_generations))\n    mutation_rate = 0.1 + (0.3 * (1 - (generation / max_generations)))  # Starts at 0.4, decays to 0.1\n\n    # Adaptive mutation parameters\n    mutation_strength = 0.5 * (1 - generation/max_generations)  # Decreases over time\n    mutation_prob = 0.3 * (1 + generation/max_generations)     # Increases slightly\n\n    # Occasionally do large mutations\n    if random.random() < 0.1:  # 10% chance for big mutation\n        mutation_rate = min(0.5, mutation_rate * 3)\n    \n    for i in range(len(individual)):\n        if random.random() < mutation_prob:##here was mutation rate:\n            step_size = max(0.2 * (1 - generation / max_generations), 1e-6)\n            individual[i] += np.random.normal(0, step_size)\n    \n    # --- Reflection Boundary Handling ---\n    min_vals = X.min(axis=0)\n    max_vals = X.max(axis=0)\n    for i in range(NUM_CLUSTERS):\n        for j in range(NUM_FEATURES):\n            if individual[i, j] < min_vals[j]:\n                individual[i, j] = min_vals[j] + (min_vals[j] - individual[i, j])\n            elif individual[i, j] > max_vals[j]:\n                individual[i, j] = max_vals[j] - (individual[i, j] - max_vals[j])\n\n    # --- K-Means Local Refinement (1 iteration) ---\n    kmeans = KMeans(n_clusters=NUM_CLUSTERS, init=individual, n_init=1, max_iter=1)\n    kmeans.fit(X)\n    refined_centroids = kmeans.cluster_centers_\n\n    return refined_centroids.flatten()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def local_search_kmeans(individual, X, n_features, fitnesses, max_iter=10):\n    \"\"\"Refine centroids using KMeans with warm-start from individual.\"\"\"\n    n_features = X.shape[1]\n    centroids = individual.reshape(NUM_CLUSTERS, NUM_FEATURES)\n\n    try:\n        kmeans = KMeans(\n            n_clusters=NUM_CLUSTERS,\n            init=centroids,\n            n_init=1,\n            max_iter=max_iter,\n            random_state=42\n        )\n        kmeans.fit(X)\n        refined = kmeans.cluster_centers_.flatten()\n\n        #Track improvement\n        before = fitnesses(individual)\n        after = fitnesses(refined)\n        print(f\"[LocalSearch] Improved fitness: {before:.4f} -> {after:.4f}\")\n        \n        return refined\n    except Exception as e:\n        print(f\"[WARN] Local search failed: {e}\")\n        return individual  # fallback in case KMeans fails\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_genetic_algorithm():\n    # Initialize population and tracking variables\n    # population = create_population(POPULATION_SIZE)\n    population = [repair_individual(ind, X) for ind in create_population(POPULATION_SIZE)]  # Apply repair during initialization\n    best_fitness_history = []\n    avg_fitness_history = []\n    valid_solutions_history = []\n    diversity_history = []\n\n    \n\n    # Initialize fitnesses as a list of fitness values\n    fitnesses = [evaluate_fitness(ind) for ind in population]\n\n    print(f\"Population size: {len(population)}, Fitnesses size: {len(fitnesses)}\")\n\n    for generation in range(max_generations):\n        # Adaptive fitness evaluation\n        apply_sharing = (generation < max_generations//2) and (generation % 5 == 0)##generation < NUM_GENERATIONS//2 thw change \n        fitnesses = [evaluate_fitness(ind, \n                      population if apply_sharing else None, \n                      apply_sharing) \n                    for ind in population]\n        # fitnesses = [evaluate_fitness(individual) for individual in population]\n        \n        # Process fitness values\n        valid_fitnesses = [f for f in fitnesses if f < PENALTY_VALUE]\n        valid_count = len(valid_fitnesses)\n        best_fit = min(valid_fitnesses) if valid_fitnesses else PENALTY_VALUE\n        avg_fit = np.mean(valid_fitnesses) if valid_fitnesses else PENALTY_VALUE\n        \n        # Track metrics\n        best_fitness_history.append(best_fit)\n        avg_fitness_history.append(avg_fit)\n        valid_solutions_history.append(valid_count)\n        \n\n        \n        \n        # Calculate diversity (moved before selection)\n        diversity_history.append(np.mean(pdist(population)))\n\n        \n        print(f\"Gen {generation+1}: Valid {valid_count}/{len(population)} | Best {-best_fit:.4f} | Avg {-avg_fit:.4f} \")\n        \n        # Tournament selection\n        selected = []\n        for _ in range(POPULATION_SIZE):\n            candidates = random.sample(range (POPULATION_SIZE), min(5, (POPULATION_SIZE)//2))\n            # Select the individual with the best (smallest) fitness\n            best_candidate_idx = min(candidates, key=lambda x: fitnesses[x])\n            selected.append(population[best_candidate_idx])\n            # selected.append(population[min(candidates, key=lambda x: fitnesses[x])])\n        \n        # Reproduction\n        next_gen = []\n        for i in range(0, len(selected) - 1, 2):\n            p1, p2 = selected[i], selected[i+1]\n            c1, c2 = crossover(p1, p2, generation, MAX_GENERATIONS)\n            c1 = mutate(c1, generation, MAX_GENERATIONS)\n            c2 = mutate(c2, generation, MAX_GENERATIONS)\n            next_gen.extend([\n                repair_individual(c1,X),\n                repair_individual(c2 ,X)\n            ])\n        \n        population = next_gen[:POPULATION_SIZE]\n\n        \n        # Diversity maintenance\n        if generation % DIVERSITY_CHECK_FREQUENCY == 0:\n            pop_array = np.array(population)\n            kmeans = KMeans(n_clusters=min(5, len(population)//2)).fit(pop_array)\n            cluster_counts = Counter(kmeans.labels_)\n            \n            for cluster_idx, count in cluster_counts.items():\n                if count > len(population)//3:\n                    to_replace = count - len(population)//3\n                    cluster_indices = np.where(kmeans.labels_ == cluster_idx)[0]\n                    replace_indices = np.random.choice(cluster_indices, to_replace, replace=False)\n                    for idx in replace_indices:\n                        population[idx] = create_individual(generation, NUM_GENERATIONS)\n\n        # fitnesses = evaluate_fitness\n        # Local search intensification\n        if generation % LOCAL_SEARCH_FREQUENCY == 0:\n            best_idx = np.argmin(fitnesses)##fitnesses\n            # population[best_idx] = local_search(population[best_idx])\n            population[best_idx] = local_search_kmeans(population[best_idx], X, NUM_CLUSTERS, evaluate_fitness)\n            fitnesses[best_idx] = evaluate_fitness(population[best_idx], population, apply_sharing)\n            \n\n            # Update fitness values after population modification\n            fitnesses = [evaluate_fitness(ind, population) for ind in population]\n\n            \n            # Replace worst individuals\n            worst_indices = np.argsort(fitnesses)[-len(population)//20:]\n            for idx in worst_indices:\n                population[idx] = create_individual(generation, NUM_GENERATIONS)\n                fitnesses[idx] = evaluate_fitness(population[idx], population, apply_sharing)  # Recalculate fitness for replaced individuals\n\n\n        # Restart mechanism\n        if generation == 50 and len(best_fitness_history) > 50:\n            if (best_fitness_history[49] - best_fitness_history[0]) < 0.05:\n                print(\"Restarting population for better exploration\")\n                elite_count = int(POPULATION_SIZE * 0.1)\n                elite_indices = np.argpartition(fitnesses, elite_count)[:elite_count]\n                elites = [population[i] for i in elite_indices]\n                population = elites + create_population(POPULATION_SIZE - elite_count, generation, NUM_GENERATIONS)\n                # Clear fitness cache after population restart\n                fitness_cache.clear()\n\n    # Final evaluation\n    best_idx = np.argmin([evaluate_fitness(ind) for ind in population])\n    best_individual = population[best_idx]\n    best_centroids = np.array(best_individual).reshape(NUM_CLUSTERS, NUM_FEATURES)\n    best_score = -min([evaluate_fitness(ind) for ind in population])  # Positive score\n\n    # Visualization\n    visualize_results(X, best_centroids, best_fitness_history, \n                    avg_fitness_history, valid_solutions_history, \n                    diversity_history, NUM_CLUSTERS, POPULATION_SIZE)\n    \n    return best_centroids, best_score\n\ndef visualize_results(X, centroids, best_fitness, avg_fitness, \n                    valid_solutions, diversity, n_clusters, pop_size):\n    \"\"\"Consolidated visualization function\"\"\"\n    plt.figure(figsize=(18, 12))\n    \n    # 1. Optimization Progress\n    plt.subplot(2, 2, 1)\n    plt.plot([-f for f in best_fitness], 'b-', label='Best Fitness')\n    plt.plot([-f for f in avg_fitness], 'g-', alpha=0.5, label='Avg Fitness')\n    plt.plot([v/pop_size for v in valid_solutions], 'r--', label='Valid Solutions')\n    plt.xlabel('Generation')\n    plt.ylabel('Score')\n    plt.title('Optimization Progress')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # 2. Diversity Tracking\n    plt.subplot(2, 2, 2)\n    plt.plot(diversity, 'm-')\n    plt.xlabel('Generation')\n    plt.ylabel('Population Diversity')\n    plt.title('Diversity Over Generations')\n    plt.grid(True, alpha=0.3)\n\n    # 3. PCA Projection\n    plt.subplot(2, 2, 3)\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    centroids_pca = pca.transform(centroids)\n    \n    colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n    labels = np.argmin(cdist(X, centroids), axis=1)\n    \n    for k in range(n_clusters):\n        plt.scatter(X_pca[labels == k, 0], X_pca[labels == k, 1],\n                    c=[colors[k]], alpha=0.6, label=f'Cluster {k+1}')\n\n    plt.scatter(centroids_pca[:,0], centroids_pca[:,1],\n                c='red', marker='X', s=200, edgecolor='black',\n                linewidth=1, label='Centroids')\n    plt.title('Final Clusters (PCA Projection)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # 4. Decision Boundaries\n    plt.subplot(2, 2, 4)\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    \n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    if X.shape[1] > 2:\n        mesh_points = np.hstack([mesh_points, np.zeros((mesh_points.shape[0], X.shape[1]-2))])\n    \n    Z = np.argmin(cdist(mesh_points, centroids), axis=1)\n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k')\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n    plt.title('Decision Boundaries')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Run the algorithm\nbest_centroids, best_score = run_genetic_algorithm()\nprint(f\"\\nBest clustering score: {best_score:.4f}\")\nprint(\"Final centroids:\\n\", best_centroids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ndef evaluate_silhouette(X, labels):\n    score = silhouette_score(X, labels)\n    print(f\"Silhouette Score: {score:.4f}\")\n    return score\n\n\n\n# Fit KMeans to the data\nkmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\nkmeans.fit(X)\n\n# Print Silhouette Score\nevaluate_silhouette(X, kmeans.labels_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import random\n# from sklearn.metrics import silhouette_score, pairwise_distances\n# from scipy.spatial.distance import cdist\n# from sklearn.cluster import KMeans\n# from collections import Counter\n# import matplotlib.pyplot as plt\n\n# # Configuration parameters\n# NUM_CLUSTERS = 3\n# POPULATION_SIZE = 75\n# NUM_GENERATIONS = 100\n# MIN_SAMPLES_PER_CLUSTER = 5\n# ELITISM_RATE = 0.15\n# TOURNAMENT_SIZE_EARLY = 3\n# TOURNAMENT_SIZE_LATE = 7\n# LOCAL_SEARCH_FREQUENCY = 50\n# DIVERSITY_CHECK_FREQUENCY = 20\n\n# def tournament_selection(population, fitnesses, generation, max_generations, elitism=0.15):\n#     \"\"\"Tournament selection with adaptive size and elitism\"\"\"\n#     selected = []\n#     elite_count = int(elitism * len(population))\n    \n#     # Elitism: select top elite_count individuals\n#     if elite_count > 0:\n#         elite_indices = np.argpartition(fitnesses, elite_count)[:elite_count]\n#         elites = [population[i] for i in elite_indices]\n#         selected.extend(elites)\n    \n#     # Adaptive tournament size\n#     tournament_size = TOURNAMENT_SIZE_EARLY\n#     if generation > max_generations * 0.7:\n#         tournament_size = TOURNAMENT_SIZE_LATE\n    \n#     # Tournament selection for the rest\n#     for _ in range(len(population) - elite_count):\n#         participants = random.sample(list(zip(population, fitnesses)), tournament_size)\n#         winner = min(participants, key=lambda x: x[1])  # Select best fitness\n#         selected.append(winner[0])\n    \n#     return selected\n\n# def crossover(parent1, parent2, generation, max_generations, alpha=0.3):\n#     \"\"\"Adaptive crossover with blending\"\"\"\n#     # Adaptive crossover rate\n#     crossover_rate = 0.9 - (0.5 * generation/max_generations)  # Decreases from 0.9 to 0.4\n    \n#     if random.random() > crossover_rate:\n#         return parent1 if random.random() < 0.5 else parent2\n    \n#     # Blend crossover\n#     child = []\n#     for p1, p2 in zip(parent1, parent2):\n#         # Blend genes within [min - α·range, max + α·range]\n#         min_val = min(p1, p2)\n#         max_val = max(p1, p2)\n#         range_val = max_val - min_val\n#         child.append(np.random.uniform(min_val - alpha * range_val, max_val + alpha * range_val))\n    \n#     return child\n\n# def mutate(individual, generation, max_generations, scale=0.1):\n#     \"\"\"Adaptive mutation with boundary handling\"\"\"\n#     individual = np.array(individual)\n    \n#     # Adaptive mutation rate\n#     mutation_rate = 0.1 + (0.3 * (1 - generation/max_generations))  # Decreases from 0.4 to 0.1\n    \n#     for i in range(len(individual)):\n#         if random.random() < mutation_rate:\n#             # Adaptive mutation step size\n#             # step_size = scale * (1 - generation/max_generations)\n#             step_size = max(0.2 * (1 - generation / max_generations), 1e-6)\n#             individual[i] += np.random.normal(0, step_size)\n    \n#     # Boundary check\n#     min_vals = np.tile(X.min(axis=0), NUM_CLUSTERS)\n#     max_vals = np.tile(X.max(axis=0), NUM_CLUSTERS)\n#     individual = np.clip(individual, min_vals, max_vals)\n    \n#     return individual.tolist()\n\n# def runn_genetic_algorithm(X):\n#     \"\"\"Main GA execution function\"\"\"\n#     # Initialize population and other variables\n#     fitness_cache = {}\n#     population = create_population(POPULATION_SIZE)\n#     best_fitness_history = []\n#     valid_solutions_history = []\n\n#     # Main GA loop\n#     for generation in range(NUM_GENERATIONS):\n#         # Evaluate fitness\n#         fitnesses = [evaluate_fitness(ind) for ind in population]\n#         best_fit = min(fitnesses)\n#         best_fitness_history.append(best_fit)\n        \n#         # Monitor valid solutions\n#         valid_count = sum(1 for f in fitnesses if f > -0.5)\n#         valid_solutions_history.append(valid_count)\n#         print(f\"Gen {generation+1}: Valid {valid_count}/{len(population)} | Best {-best_fit:.4f}\")\n\n#         # Selection\n#         selected_population = tournament_selection(\n#             population, fitnesses, generation, NUM_GENERATIONS, ELITISM_RATE\n#         )\n\n#         # Crossover + Mutation\n#         next_generation = []\n#         for i in range(0, len(selected_population) - 1, 2):\n#             parent1, parent2 = selected_population[i], selected_population[i+1]\n#             child1 = crossover(parent1, parent2, generation, NUM_GENERATIONS)\n#             child2 = crossover(parent2, parent1, generation, NUM_GENERATIONS)\n            \n#             # Adaptive mutation scale\n#             mutation_scale = 0.05 if generation > NUM_GENERATIONS*0.7 else 0.1\n#             next_generation.extend([\n#                 mutate(child1, generation, NUM_GENERATIONS, scale=mutation_scale),\n#                 mutate(child2, generation, NUM_GENERATIONS, scale=mutation_scale)\n#             ])\n        \n#         if len(selected_population) % 2 != 0:\n#             next_generation.append(mutate(selected_population[-1], generation, NUM_GENERATIONS))\n        \n#         population = next_generation[:POPULATION_SIZE]\n\n#         # Diversity maintenance\n#         if valid_count < 0.8 * len(population):\n#             invalid_indices = np.argsort(fitnesses)[-len(population)//10:]\n#             for idx in invalid_indices:\n#                 population[idx] = create_individual(generation, NUM_GENERATIONS)\n#             print(f\"Injected diversity at gen {generation+1}\")\n        \n#         # Late-stage intensification\n#         if generation == int(NUM_GENERATIONS*0.7):\n#             print(\"Activating intense optimization mode\")\n#             current_tournament_size = 7\n#             elite_indices = np.argsort(fitnesses)[:10]\n#             for i in elite_indices:\n#                 population[i] = mutate(population[i], generation, NUM_GENERATIONS, scale=0.05)\n#             elite_indices = np.argsort(fitnesses)[:5]\n#             elites = [mutate(population[i], generation, NUM_GENERATIONS, scale=0.02) for i in elite_indices]\n#             population[-5:] = elites\n\n#     # Final results with enhanced visualization\n#     print(\"\\n=== Optimization Complete ===\")\n#     plt.figure(figsize=(14, 6))\n\n#     # 1. Fitness Progress Plot (Left)\n#     plt.subplot(121)\n#     plt.plot([-f for f in best_fitness_history], 'b-', label='Best Silhouette', linewidth=2)\n#     plt.plot([v/len(population)*100 for v in valid_solutions_history], 'g--', label='Valid Solutions (%)', alpha=0.7)\n#     plt.xlabel('Generation', fontsize=12)\n#     plt.ylabel('Score', fontsize=12)\n#     plt.title('Optimization Progress', fontsize=14)\n#     plt.legend(fontsize=10)\n#     plt.grid(True, alpha=0.3)\n\n#     # 2. Cluster Visualization (Right)\n#     plt.subplot(122)\n#     best_centroids = np.array(population[np.argmin(fitnesses)]).reshape(NUM_CLUSTERS, -1)\n#     labels = np.argmin(cdist(X, best_centroids), axis=1)\n\n#     # Use distinct colors and markers\n#     colors = plt.cm.viridis(np.linspace(0, 1, NUM_CLUSTERS))\n#     for k in range(NUM_CLUSTERS):\n#         cluster_points = X[labels == k]\n#         plt.scatter(cluster_points[:,0], cluster_points[:,1], \n#                    c=[colors[k]], alpha=0.6, label=f'Cluster {k+1}')\n        \n#     # Plot centroids with outlines\n#     plt.scatter(best_centroids[:,0], best_centroids[:,1],\n#                c='red', marker='X', s=200, edgecolor='black',\n#                linewidth=2, label='Centroids')\n\n#     plt.title(f'Final Clusters (Silhouette: {-best_fit:.4f})', fontsize=14)\n#     plt.xlabel('Feature 1', fontsize=12)\n#     plt.ylabel('Feature 2', fontsize=12)\n#     plt.legend(fontsize=10)\n#     plt.grid(True, alpha=0.3)\n\n#     plt.tight_layout()\n#     plt.show()\n\n#     # Print numerical results\n#     print(f\"\\nOptimal Centroids (Silhouette Score: {-best_fit:.4f}):\")\n#     print(best_centroids)\n\n#     return best_centroids, -best_fit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Now you can call the function and get the results:\n# best_centroids, best_score = runn_genetic_algorithm(X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *EVALUATING MODELS*","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(X_scaled.shape)  # Should show (n_samples, n_features)\n# print(labels.shape)    # Should show (n_samples,)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n\n# After getting your final clusters\nlabels = np.argmin(cdist(X, best_centroids), axis=1)\n\n# Silhouette Score (-1 to 1, higher is better)\nsil_score = silhouette_score(X, labels)\nprint(f\"Silhouette Score: {sil_score:.4f}\")\n\n# Davies-Bouldin Index (0 to ∞, lower is better)\ndb_index = davies_bouldin_score(X, labels)\nprint(f\"Davies-Bouldin Index: {db_index:.4f}\")\n\n# Calinski-Harabasz Index (higher is better)\nch_index = calinski_harabasz_score(X, labels)\nprint(f\"Calinski-Harabasz Index: {ch_index:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Run multiple times and compare results\n# n_runs = 2\n# results = []\n# for _ in range(n_runs):\n#     centroids, score = run_genetic_algorithm(X)\n#     labels = np.argmin(cdist(X, centroids), axis=1)\n#     results.append(labels)\n\n# # Compare using Adjusted Rand Index\n# from sklearn.metrics import adjusted_rand_score\n# for i in range(n_runs-1):\n#     print(f\"Consistency between run {i} and {i+1}:\",\n#           adjusted_rand_score(results[i], results[i+1]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Cluster sizes\n# cluster_sizes = np.bincount(labels)\n# print(\"Cluster sizes:\", cluster_sizes)\n\n# # Intra-cluster distances\n# intra_dists = [np.mean(pairwise_distances(X[labels==i])) \n#                for i in range(NUM_CLUSTERS)]\n# print(\"Mean intra-cluster distances:\", intra_dists)\n\n# # Inter-cluster distances\n# inter_dists = pairwise_distances(best_centroids)\n# print(\"Inter-centroid distances:\\n\", inter_dists)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Compare feature distributions across clusters\n# fig, axes = plt.subplots(NUM_FEATURES, 1, figsize=(10, NUM_FEATURES*3))\n# for i in range(NUM_FEATURES):\n#     for cluster in range(NUM_CLUSTERS):\n#         axes[i].hist(X[labels==cluster, i], alpha=0.5, \n#                     label=f'Cluster {cluster}')\n#     axes[i].set_title(f'Feature {i} Distribution')\n#     axes[i].legend()\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans, DBSCAN\n\n# Compare with KMeans\nkmeans = KMeans(n_clusters=NUM_CLUSTERS).fit(X)\nprint(\"KMeans Silhouette:\", silhouette_score(X, kmeans.labels_))\n\n# Compare with DBSCAN (if appropriate for your data)\ndbscan = DBSCAN(eps=0.5, min_samples=5).fit(X)\nif len(np.unique(dbscan.labels_)) > 1:\n    print(\"DBSCAN Silhouette:\", silhouette_score(X, dbscan.labels_))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\ndef evaluate_silhouette(X, labels):\n    score = silhouette_score(X, labels)\n    print(f\"Silhouette Score: {score:.4f}\")\n    return score\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Assuming X is your dataset and NUM_CLUSTERS is defined\nkmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\nkmeans.fit(X)\n\n# Print Silhouette Score\nevaluate_silhouette(X, kmeans.labels_)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ndef evaluate_silhouette(X, labels):\n    score = silhouette_score(X, labels)\n    print(f\"Silhouette Score: {score:.4f}\")\n    return score\n\n\n\n# Fit KMeans to the data\nkmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\nkmeans.fit(X)\n\n# Print Silhouette Score\nevaluate_silhouette(X, kmeans.labels_)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}